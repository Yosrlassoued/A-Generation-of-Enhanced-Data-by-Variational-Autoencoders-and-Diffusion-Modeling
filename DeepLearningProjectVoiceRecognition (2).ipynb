{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Recognition and Data Augmentation using Diffusion Models\n",
    "\n",
    "This notebook implements the methodology described in the paper 'A Generation of Enhanced Data by Variational Autoencoders and Diffusion Modeling' (Electronics 2024, 13, 1314).\n",
    "\n",
    "**Pipeline:**\n",
    "1. Data Loading and Preprocessing (EmoDB, RAVDESS) -> Mel-Spectrograms\n",
    "2. ResNet-50 Emotion Embedding Model Training and Feature Extraction (PyTorch)\n",
    "3. Diffusion Model for Mel-Spectrogram Generation (PyTorch, U-Net based)\n",
    "4. Data Augmentation using the Diffusion Model\n",
    "5. Evaluation of Original vs. Augmented Data (WA, UA, Confusion Matrix, P/R/F1)\n",
    "\n",
    "**Notes for Google Colab:**\n",
    "*   Use a GPU runtime (`Runtime` -> `Change runtime type`).\n",
    "*   Training (especially diffusion) is computationally expensive and time-consuming.\n",
    "*   Mount Google Drive to save datasets, models, and generated data persistently (see next cell)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "imports"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in c:\\users\\youssef\\documents\\4eme2425\\deeplearning\\emotion reconigtion project\\my_audio_env\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\youssef\\documents\\4eme2425\\deeplearning\\emotion reconigtion project\\my_audio_env\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: requests in c:\\users\\youssef\\documents\\4eme2425\\deeplearning\\emotion reconigtion project\\my_audio_env\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\youssef\\documents\\4eme2425\\deeplearning\\emotion reconigtion project\\my_audio_env\\lib\\site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\youssef\\documents\\4eme2425\\deeplearning\\emotion reconigtion project\\my_audio_env\\lib\\site-packages (from librosa) (0.61.2)\n",
      "Requirement already satisfied: numpy>=1.22.3 in c:\\users\\youssef\\documents\\4eme2425\\deeplearning\\emotion reconigtion project\\my_audio_env\\lib\\site-packages (from librosa) (2.2.5)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\youssef\\documents\\4eme2425\\deeplearning\\emotion reconigtion project\\my_audio_env\\lib\\site-packages (from librosa) (1.15.3)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in c:\\users\\youssef\\documents\\4eme2425\\deeplearning\\emotion reconigtion project\\my_audio_env\\lib\\site-packages (from librosa) (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\youssef\\documents\\4eme2425\\deeplearning\\emotion reconigtion project\\my_audio_env\\lib\\site-packages (from librosa) (1.5.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\youssef\\documents\\4eme2425\\deeplearning\\emotion reconigtion project\\my_audio_env\\lib\\site-packages (from librosa) (5.2.1)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in c:\\users\\youssef\\documents\\4eme2425\\deeplearning\\emotion reconigtion project\\my_audio_env\\lib\\site-packages (from librosa) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in c:\\users\\youssef\\documents\\4eme2425\\deeplearning\\emotion reconigtion project\\my_audio_env\\lib\\site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in c:\\users\\youssef\\documents\\4eme2425\\deeplearning\\emotion reconigtion project\\my_audio_env\\lib\\site-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in c:\\users\\youssef\\documents\\4eme2425\\deeplearning\\emotion reconigtion project\\my_audio_env\\lib\\site-packages (from librosa) (4.13.2)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in c:\\users\\youssef\\documents\\4eme2425\\deeplearning\\emotion reconigtion project\\my_audio_env\\lib\\site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\youssef\\documents\\4eme2425\\deeplearning\\emotion reconigtion project\\my_audio_env\\lib\\site-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\youssef\\documents\\4eme2425\\deeplearning\\emotion reconigtion project\\my_audio_env\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\youssef\\documents\\4eme2425\\deeplearning\\emotion reconigtion project\\my_audio_env\\lib\\site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\youssef\\documents\\4eme2425\\deeplearning\\emotion reconigtion project\\my_audio_env\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\youssef\\documents\\4eme2425\\deeplearning\\emotion reconigtion project\\my_audio_env\\lib\\site-packages (from requests) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\youssef\\documents\\4eme2425\\deeplearning\\emotion reconigtion project\\my_audio_env\\lib\\site-packages (from requests) (2025.4.26)\n",
      "Requirement already satisfied: packaging in c:\\users\\youssef\\documents\\4eme2425\\deeplearning\\emotion reconigtion project\\my_audio_env\\lib\\site-packages (from lazy_loader>=0.1->librosa) (25.0)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in c:\\users\\youssef\\documents\\4eme2425\\deeplearning\\emotion reconigtion project\\my_audio_env\\lib\\site-packages (from numba>=0.51.0->librosa) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\youssef\\documents\\4eme2425\\deeplearning\\emotion reconigtion project\\my_audio_env\\lib\\site-packages (from pooch>=1.1->librosa) (4.3.8)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\youssef\\documents\\4eme2425\\deeplearning\\emotion reconigtion project\\my_audio_env\\lib\\site-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\youssef\\documents\\4eme2425\\deeplearning\\emotion reconigtion project\\my_audio_env\\lib\\site-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\youssef\\documents\\4eme2425\\deeplearning\\emotion reconigtion project\\my_audio_env\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "!pip install librosa tqdm requests \n",
    "\n",
    "import os\n",
    "import glob\n",
    "import zipfile\n",
    "import requests\n",
    "import shutil\n",
    "import random\n",
    "from tqdm.notebook import tqdm  \n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, balanced_accuracy_score\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Configuration and Constants\n",
    "\n",
    "**Important:** If you mounted Google Drive, modify the `*_DIR` paths below to point to your Drive folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "config"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Directory: /content\\data\n",
      "Model Save Directory: /content\\models\n"
     ]
    }
   ],
   "source": [
    "# --- Data Parameters ---\n",
    "TARGET_SAMPLE_RATE = 22050\n",
    "DURATION_SECONDS = 10  # Pad/truncate audio to this length\n",
    "TARGET_SAMPLES = TARGET_SAMPLE_RATE * DURATION_SECONDS\n",
    "\n",
    "# --- Mel-Spectrogram Parameters ---\n",
    "N_FFT = 1024        # Window size for STFT\n",
    "HOP_LENGTH = 256    # Hop length for STFT\n",
    "N_MELS = 80         # Number of Mel bands\n",
    "EXPECTED_TIME_STEPS = 862 # Fixed size after padding/truncating spectrograms\n",
    "\n",
    "# --- Emotion Labels ---\n",
    "EMOTIONS = {\n",
    "    \"neutral\": 0,\n",
    "    \"happy\": 1,\n",
    "    \"sad\": 2,\n",
    "    \"angry\": 3,\n",
    "    \"fear\": 4,  # EmoDB 'Angst', RAVDESS 'fearful'\n",
    "    \"disgust\": 5\n",
    "}\n",
    "NUM_EMOTIONS = len(EMOTIONS)\n",
    "\n",
    "# --- Model Training Parameters ---\n",
    "# Reduce epochs significantly for initial testing in Colab!\n",
    "RESNET_EPOCHS = 50 # Paper uses 800, reduce for faster demo\n",
    "RESNET_BATCH_SIZE = 32\n",
    "RESNET_LEARNING_RATE = 1e-4\n",
    "\n",
    "DIFFUSION_EPOCHS = 100 # Paper doesn't specify, GUESS - likely needs more. Reduce for testing.\n",
    "DIFFUSION_BATCH_SIZE = 16 # Diffusion models often need smaller batches\n",
    "DIFFUSION_LEARNING_RATE = 1e-4\n",
    "DIFFUSION_TIMESTEPS = 1000 # Number of noise levels\n",
    "\n",
    "# --- Paths --- \n",
    "# !! MODIFY THESE IF USING GOOGLE DRIVE !!\n",
    "BASE_DIR = \"/content\" # Default Colab storage\n",
    "# Example if using Drive:\n",
    "# DRIVE_BASE_PATH = '/content/drive/MyDrive/Colab_Emotion_Augmentation'\n",
    "# BASE_DIR = DRIVE_BASE_PATH \n",
    "\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "EMODB_URL = \"http://emodb.bilderbar.info/download/download.zip\" # Official EmoDB download URL might require navigation\n",
    "EMODB_DIR = os.path.join(DATA_DIR, \"emodb\")\n",
    "RAVDESS_URL = \"https://zenodo.org/record/1188976/files/Audio_Speech_Actors_01-24.zip?download=1\" # Example URL, check for official source\n",
    "RAVDESS_DIR = os.path.join(DATA_DIR, \"ravdess\")\n",
    "PROCESSED_DIR = os.path.join(DATA_DIR, \"processed_mels\")\n",
    "AUGMENTED_DIR = os.path.join(DATA_DIR, \"augmented_mels\")\n",
    "MODEL_SAVE_DIR = os.path.join(BASE_DIR, \"models\") # Save models outside data dir\n",
    "\n",
    "RESNET_MODEL_PATH = os.path.join(MODEL_SAVE_DIR, \"resnet50_emotion_embedder.pth\")\n",
    "DIFFUSION_MODEL_PATH = os.path.join(MODEL_SAVE_DIR, \"diffusion_mel_generator.pth\")\n",
    "\n",
    "# --- Create Directories ---\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(EMODB_DIR, exist_ok=True) # Create dataset dirs explicitly\n",
    "os.makedirs(RAVDESS_DIR, exist_ok=True)\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "os.makedirs(AUGMENTED_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Data Directory: {DATA_DIR}\")\n",
    "print(f\"Model Save Directory: {MODEL_SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Helper Functions for Downloading and Extracting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "download-helpers"
   },
   "outputs": [],
   "source": [
    "def download_file(url, filename):\n",
    "    \"\"\"Downloads a file from a URL to a local filename.\"\"\"\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"File already exists: {filename}\")\n",
    "        return True\n",
    "    print(f\"Downloading {url} to {filename}...\")\n",
    "    try:\n",
    "        with requests.get(url, stream=True) as r:\n",
    "            r.raise_for_status()\n",
    "            total_size = int(r.headers.get('content-length', 0))\n",
    "            block_size = 8192 # 8KB chunks\n",
    "            progress_bar = tqdm(total=total_size, unit='iB', unit_scale=True)\n",
    "            with open(filename, 'wb') as f:\n",
    "                for chunk in r.iter_content(chunk_size=block_size):\n",
    "                    progress_bar.update(len(chunk))\n",
    "                    f.write(chunk)\n",
    "            progress_bar.close()\n",
    "        if total_size != 0 and progress_bar.n != total_size:\n",
    "             print(\"ERROR, something went wrong during download\")\n",
    "             return False\n",
    "        print(f\"Download complete: {filename}\")\n",
    "        return True\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading {url}: {e}\")\n",
    "        if os.path.exists(filename):\n",
    "            os.remove(filename) # Clean up incomplete download\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during download: {e}\")\n",
    "        if os.path.exists(filename):\n",
    "            os.remove(filename)\n",
    "        return False\n",
    "\n",
    "def extract_zip(zip_path, extract_to):\n",
    "    \"\"\"Extracts a zip file.\"\"\"\n",
    "    if not os.path.exists(zip_path):\n",
    "        print(f\"Zip file not found: {zip_path}\")\n",
    "        return False\n",
    "    \n",
    "    # Check if extraction directory seems populated (simple check based on expected content)\n",
    "    expected_emodb = os.path.join(extract_to, \"wav\") # EmoDB has a 'wav' subfolder\n",
    "    expected_ravdess = os.path.join(extract_to, \"Actor_01\") # RAVDESS has Actor_ folders\n",
    "    already_extracted = False\n",
    "    if os.path.exists(extract_to):\n",
    "        if zip_path.endswith(\"emodb.zip\") and os.path.exists(expected_emodb) and len(os.listdir(expected_emodb)) > 0:\n",
    "            already_extracted = True\n",
    "        elif zip_path.endswith(\"ravdess.zip\") and os.path.exists(expected_ravdess) and len(os.listdir(expected_ravdess)) > 0:\n",
    "             already_extracted = True\n",
    "             \n",
    "    if already_extracted:\n",
    "         print(f\"Directory already exists and seems populated: {extract_to}\")\n",
    "         return True\n",
    "    else:\n",
    "        print(f\"Directory {extract_to} empty or incomplete. Extracting.\")\n",
    "\n",
    "    print(f\"Extracting {zip_path} to {extract_to}...\")\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            for member in tqdm(zip_ref.infolist(), desc='Extracting '):\n",
    "                 try:\n",
    "                     # Prevent extracting __MACOSX folders if they exist\n",
    "                     if \"__MACOSX\" not in member.filename:\n",
    "                        zip_ref.extract(member, extract_to)\n",
    "                 except zipfile.error as e:\n",
    "                     print(f\"Error extracting member {member.filename}: {e}\")\n",
    "            # zip_ref.extractall(extract_to) # Less informative progress\n",
    "        print(f\"Extraction complete: {extract_to}\")\n",
    "        # Clean up zip file after successful extraction\n",
    "        # os.remove(zip_path)\n",
    "        return True\n",
    "    except zipfile.BadZipFile:\n",
    "        print(f\"Error: Bad zip file: {zip_path}\")\n",
    "        # Optionally remove the bad zip file\n",
    "        # os.remove(zip_path)\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during extraction: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Download and Extract Datasets\n",
    "**Note:** Direct download URLs might change or require specific permissions. If downloads fail, please download the datasets manually and place them in the `DATA_DIR` (`emodb` and `ravdess` subfolders respectively), or upload them to your mounted Google Drive and adjust paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "download-datasets"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- EmoDB --- \n",
      "EmoDB data not found.\n",
      "Please download 'download.zip' manually from http://emodb.bilderbar.info/download/\n",
      "Extract it, find the 'wav' folder, and place its contents into '/content\\data\\emodb\\wav'.\n",
      "Directory created: /content\\data\\emodb\\wav\n",
      "\n",
      "--- RAVDESS --- \n",
      "RAVDESS data not found. Attempting download...\n",
      "Downloading https://zenodo.org/record/1188976/files/Audio_Speech_Actors_01-24.zip?download=1 to /content\\data\\ravdess.zip...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ca2fa46f3bc4c6d82745279d7b383f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/208M [00:00<?, ?iB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete: /content\\data\\ravdess.zip\n",
      "Directory /content\\data\\ravdess empty or incomplete. Extracting.\n",
      "Extracting /content\\data\\ravdess.zip to /content\\data\\ravdess...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d798b50e6374bfc9171401591edd4ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting :   0%|          | 0/1464 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction complete: /content\\data\\ravdess\n"
     ]
    }
   ],
   "source": [
    "# --- EmoDB Download --- \n",
    "# EmoDB download often requires navigating the website.\n",
    "# Provide instructions for manual download.\n",
    "emodb_zip_path = os.path.join(DATA_DIR, \"emodb.zip\")\n",
    "emodb_wav_dir = os.path.join(EMODB_DIR, \"wav\")\n",
    "print(\"--- EmoDB --- \")\n",
    "if not os.path.exists(emodb_wav_dir) or not os.listdir(emodb_wav_dir):\n",
    "     print(\"EmoDB data not found.\")\n",
    "     print(\"Please download 'download.zip' manually from http://emodb.bilderbar.info/download/\")\n",
    "     print(f\"Extract it, find the 'wav' folder, and place its contents into '{emodb_wav_dir}'.\")\n",
    "     # Create the target directory if it doesn't exist, so the user knows where to put files\n",
    "     os.makedirs(emodb_wav_dir, exist_ok=True)\n",
    "     print(f\"Directory created: {emodb_wav_dir}\")\n",
    "     # Example: After downloading and extracting, you might have './download/wav'. \n",
    "     # You need to move/copy the *.wav files from there into the EMODB_DIR/wav folder defined above.\n",
    "else:\n",
    "    print(f\"EmoDB data seems to be present in {emodb_wav_dir}.\")\n",
    "\n",
    "# --- RAVDESS Download ---\n",
    "ravdess_zip_path = os.path.join(DATA_DIR, \"ravdess.zip\")\n",
    "ravdess_actor_dir = os.path.join(RAVDESS_DIR, \"Actor_01\") # Check for existence of Actor_01\n",
    "print(\"\\n--- RAVDESS --- \")\n",
    "if not os.path.exists(ravdess_actor_dir):\n",
    "    print(\"RAVDESS data not found. Attempting download...\")\n",
    "    if download_file(RAVDESS_URL, ravdess_zip_path):\n",
    "        # RAVDESS often extracts into the Actor_ folders directly in the target dir\n",
    "        extract_zip(ravdess_zip_path, RAVDESS_DIR)\n",
    "    else:\n",
    "        print(\"RAVDESS download failed.\")\n",
    "        print(\"Please download the audio dataset manually (e.g., search 'RAVDESS audio dataset', often on Zenodo/Kaggle)\")\n",
    "        print(f\"Extract the zip file. It should contain folders like 'Actor_01', 'Actor_02', etc.\") \n",
    "        print(f\"Place these 'Actor_XX' folders directly inside '{RAVDESS_DIR}'.\")\n",
    "        # Create the target directory if it doesn't exist\n",
    "        os.makedirs(RAVDESS_DIR, exist_ok=True)\n",
    "else:\n",
    "    print(f\"RAVDESS data seems to be present in {RAVDESS_DIR}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Audio Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "audio-proc-funcs"
   },
   "outputs": [],
   "source": [
    "def load_and_preprocess_audio(file_path, target_sr, target_samples):\n",
    "    \"\"\"Loads, resamples, and pads/truncates audio.\"\"\"\n",
    "    try:\n",
    "        # Load audio file\n",
    "        wav, sr = librosa.load(file_path, sr=None) # Load original sample rate\n",
    "\n",
    "        # Resample if necessary\n",
    "        if sr != target_sr:\n",
    "            wav = librosa.resample(wav, orig_sr=sr, target_sr=target_sr)\n",
    "            sr = target_sr # Update sample rate variable\n",
    "            \n",
    "        # Ensure mono\n",
    "        if wav.ndim > 1:\n",
    "            wav = librosa.to_mono(wav)\n",
    "\n",
    "        # Pad or truncate\n",
    "        if len(wav) < target_samples:\n",
    "            padding = target_samples - len(wav)\n",
    "            left_pad = padding // 2\n",
    "            right_pad = padding - left_pad\n",
    "            wav = np.pad(wav, (left_pad, right_pad), mode='constant')\n",
    "        elif len(wav) > target_samples:\n",
    "            wav = wav[:target_samples]\n",
    "\n",
    "        # Final length check\n",
    "        if len(wav) != target_samples:\n",
    "             # This might happen due to rounding, adjust slightly\n",
    "             if len(wav) < target_samples:\n",
    "                 wav = np.pad(wav, (0, target_samples - len(wav)), mode='constant')\n",
    "             else:\n",
    "                 wav = wav[:target_samples]\n",
    "\n",
    "        return wav\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def wav_to_mel_spectrogram(wav, sr, n_fft, hop_length, n_mels):\n",
    "    \"\"\"Converts waveform to mel spectrogram.\"\"\"\n",
    "    try:\n",
    "        mel_spec = librosa.feature.melspectrogram(\n",
    "            y=wav, \n",
    "            sr=sr, \n",
    "            n_fft=n_fft, \n",
    "            hop_length=hop_length, \n",
    "            n_mels=n_mels\n",
    "        )\n",
    "        # Convert to dB scale\n",
    "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        return mel_spec_db\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting waveform to mel spectrogram: {e}\")\n",
    "        return None\n",
    "\n",
    "def normalize_spectrogram(mel_spec):\n",
    "    \"\"\"Applies Z-score normalization to a spectrogram.\"\"\"\n",
    "    if mel_spec is None:\n",
    "        return None\n",
    "    mean = np.mean(mel_spec)\n",
    "    std = np.std(mel_spec)\n",
    "    if std < 1e-6: # Avoid division by zero/very small numbers for silent spectrograms\n",
    "        return mel_spec - mean # Just center it\n",
    "    normalized_spec = (mel_spec - mean) / std\n",
    "    return normalized_spec\n",
    "\n",
    "def pad_truncate_spectrogram(spec, target_time_steps):\n",
    "    \"\"\"Pads or truncates the time dimension of a spectrogram.\"\"\"\n",
    "    if spec is None:\n",
    "        return None\n",
    "    current_time_steps = spec.shape[1]\n",
    "    if current_time_steps == target_time_steps:\n",
    "        return spec\n",
    "    elif current_time_steps < target_time_steps:\n",
    "        padding = target_time_steps - current_time_steps\n",
    "        # Pad on the right side with a value representing silence (e.g., min value or slightly lower)\n",
    "        pad_value = np.min(spec) if spec.size > 0 else -80.0 # Use min value or default low dB\n",
    "        spec = np.pad(spec, ((0, 0), (0, padding)), mode='constant', constant_values=pad_value) \n",
    "    elif current_time_steps > target_time_steps:\n",
    "        spec = spec[:, :target_time_steps] # Truncate from the right\n",
    "    return spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Parsing Filenames and Extracting Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "label-parsing"
   },
   "outputs": [],
   "source": [
    "def get_emotion_label(file_path, dataset_name):\n",
    "    \"\"\"Extracts emotion label from filename based on dataset conventions.\"\"\"\n",
    "    filename = os.path.basename(file_path)\n",
    "    try:\n",
    "        if dataset_name == \"emodb\":\n",
    "            # EmoDB: 03a01W.wav -> W = Anger\n",
    "            # Emotion codes: W(Anger), L(Boredom), E(Disgust), A(Fear), F(Happy), T(Sad), N(Neutral)\n",
    "            code = filename[5] # 6th character (0-indexed)\n",
    "            mapping = {\n",
    "                'W': \"angry\", \n",
    "                'L': \"neutral\", # Mapping Boredom to Neutral for simplicity / based on target labels\n",
    "                'E': \"disgust\", \n",
    "                'A': \"fear\", \n",
    "                'F': \"happy\", \n",
    "                'T': \"sad\", \n",
    "                'N': \"neutral\"\n",
    "            }\n",
    "            emotion = mapping.get(code)\n",
    "            # Return only emotions in our target set\n",
    "            return emotion if emotion in EMOTIONS else None\n",
    "\n",
    "        elif dataset_name == \"ravdess\":\n",
    "            # RAVDESS: 03-01-01-01-01-01-01.wav -> 3rd part is emotion\n",
    "            # Codes: 01(neutral), 02(calm), 03(happy), 04(sad), 05(angry), 06(fearful), 07(disgust), 08(surprised)\n",
    "            parts = filename.split('-')\n",
    "            if len(parts) < 3:\n",
    "                return None # Invalid filename format\n",
    "            code = int(parts[2])\n",
    "            mapping = {\n",
    "                1: \"neutral\", \n",
    "                2: \"neutral\", # Mapping Calm to Neutral\n",
    "                3: \"happy\", \n",
    "                4: \"sad\", \n",
    "                5: \"angry\", \n",
    "                6: \"fear\", \n",
    "                7: \"disgust\", \n",
    "                8: None # \"surprised\" - Excluded based on target EMOTIONS dict\n",
    "            }\n",
    "            emotion = mapping.get(code)\n",
    "            # Return only emotions in our target set\n",
    "            return emotion if emotion in EMOTIONS else None\n",
    "        else:\n",
    "            return None\n",
    "    except (IndexError, ValueError, KeyError) as e:\n",
    "        print(f\"Error parsing filename {filename} for {dataset_name}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Process All Audio Files and Save Mel-Spectrograms\n",
    "\n",
    "This step iterates through the downloaded audio files, applies the preprocessing steps (load, resample, pad, convert to mel-spectrogram, normalize, fix time steps), and saves the resulting spectrograms as `.npy` files in the `PROCESSED_DIR`. It also creates a metadata list mapping file paths to integer labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "process-datasets"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed metadata file not found at /content\\data\\processed_metadata.npy. Processing datasets...\n",
      "\n",
      "Processing emodb dataset from /content\\data\\emodb...\n",
      "Found 0 audio files for emodb.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83856bae3ed5490c88ea1279fed7d72d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing emodb: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing emodb. Saved 0 spectrograms.\n",
      "\n",
      "Processing ravdess dataset from /content\\data\\ravdess...\n",
      "Found 1440 audio files for ravdess.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0f4da84a1f34b3b87d4fa457d4e7726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing ravdess:   0%|          | 0/1440 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing ravdess. Saved 1248 spectrograms.\n",
      "\n",
      "Total processed files: 1248\n",
      "Saved metadata to /content\\data\\processed_metadata.npy\n",
      "\n",
      "Example metadata entry: ('/content\\\\data\\\\processed_mels\\\\ravdess_03-01-01-01-01-01-01.npy', 0)\n",
      "Example spectrogram shape: (80, 862)\n"
     ]
    }
   ],
   "source": [
    "all_files_metadata = [] # List to store tuples of (mel_path, emotion_label_int)\n",
    "\n",
    "def process_dataset(dataset_dir, dataset_name, processed_dir):\n",
    "    \"\"\"Processes all audio files in a dataset directory.\"\"\"\n",
    "    print(f\"\\nProcessing {dataset_name} dataset from {dataset_dir}...\")\n",
    "    dataset_metadata = []\n",
    "    audio_files = []\n",
    "    \n",
    "    if not os.path.isdir(dataset_dir):\n",
    "        print(f\"Error: Dataset directory not found: {dataset_dir}\")\n",
    "        return []\n",
    "        \n",
    "    if dataset_name == \"emodb\":\n",
    "        # EmoDB files are directly in the 'wav' subfolder\n",
    "        wav_dir = os.path.join(dataset_dir, \"wav\")\n",
    "        if os.path.exists(wav_dir):\n",
    "             audio_files = glob.glob(os.path.join(wav_dir, \"*.wav\"))\n",
    "        else:\n",
    "             print(f\"Warning: EmoDB 'wav' directory not found at {wav_dir}\")\n",
    "    elif dataset_name == \"ravdess\":\n",
    "        # RAVDESS files are in Actor_XX subfolders\n",
    "        actor_dirs = glob.glob(os.path.join(dataset_dir, \"Actor_*\"))\n",
    "        if not actor_dirs:\n",
    "             print(f\"Warning: RAVDESS 'Actor_*' directories not found in {dataset_dir}\")\n",
    "        for actor_dir in actor_dirs:\n",
    "            # Check if actor_dir is actually a directory before globbing\n",
    "            if os.path.isdir(actor_dir):\n",
    "                 audio_files.extend(glob.glob(os.path.join(actor_dir, \"*.wav\")))\n",
    "            else:\n",
    "                 print(f\"Warning: Found item '{actor_dir}' which is not a directory.\")\n",
    "\n",
    "    print(f\"Found {len(audio_files)} audio files for {dataset_name}.\")\n",
    "\n",
    "    processed_count = 0\n",
    "    for file_path in tqdm(audio_files, desc=f\"Processing {dataset_name}\"):\n",
    "        emotion_label_str = get_emotion_label(file_path, dataset_name)\n",
    "\n",
    "        # Skip files with emotions not in our target set or parsing errors\n",
    "        if emotion_label_str is None:\n",
    "            continue\n",
    "\n",
    "        # --- Perform Preprocessing ---\n",
    "        # 1. Load, resample, pad/truncate audio\n",
    "        wav = load_and_preprocess_audio(file_path, TARGET_SAMPLE_RATE, TARGET_SAMPLES)\n",
    "        if wav is None: continue\n",
    "\n",
    "        # 2. Convert to mel spectrogram\n",
    "        mel_spec = wav_to_mel_spectrogram(wav, TARGET_SAMPLE_RATE, N_FFT, HOP_LENGTH, N_MELS)\n",
    "        if mel_spec is None: continue\n",
    "\n",
    "        # 3. Normalize spectrogram\n",
    "        normalized_spec = normalize_spectrogram(mel_spec)\n",
    "        if normalized_spec is None: continue\n",
    "\n",
    "        # 4. Pad/Truncate spectrogram time dimension to fixed size\n",
    "        final_spec = pad_truncate_spectrogram(normalized_spec, EXPECTED_TIME_STEPS)\n",
    "        if final_spec is None: continue\n",
    "\n",
    "        # Check final shape\n",
    "        if final_spec.shape != (N_MELS, EXPECTED_TIME_STEPS):\n",
    "             print(f\"Warning: Spectrogram shape mismatch for {file_path}. Got {final_spec.shape}, expected {(N_MELS, EXPECTED_TIME_STEPS)}. Skipping.\")\n",
    "             continue\n",
    "\n",
    "        # --- Save Processed Spectrogram ---\n",
    "        base_filename = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        save_filename = f\"{dataset_name}_{base_filename}.npy\"\n",
    "        save_path = os.path.join(processed_dir, save_filename)\n",
    "\n",
    "        try:\n",
    "            np.save(save_path, final_spec.astype(np.float32)) # Save as float32\n",
    "\n",
    "            # Store metadata\n",
    "            emotion_label_int = EMOTIONS[emotion_label_str]\n",
    "            dataset_metadata.append((save_path, emotion_label_int))\n",
    "            processed_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving spectrogram {save_path}: {e}\")\n",
    "\n",
    "    print(f\"Finished processing {dataset_name}. Saved {processed_count} spectrograms.\")\n",
    "    return dataset_metadata\n",
    "\n",
    "# --- Run Processing --- \n",
    "metadata_path = os.path.join(DATA_DIR, \"processed_metadata.npy\")\n",
    "\n",
    "# Check if processing is already done by looking for the metadata file\n",
    "if not os.path.exists(metadata_path):\n",
    "    print(f\"Processed metadata file not found at {metadata_path}. Processing datasets...\")\n",
    "    # Ensure the processed directory exists and is empty if re-running\n",
    "    if os.path.exists(PROCESSED_DIR) and os.listdir(PROCESSED_DIR):\n",
    "         print(f\"Warning: Processed directory {PROCESSED_DIR} is not empty. Files might be overwritten.\")\n",
    "         # Optionally clear the directory:\n",
    "         # shutil.rmtree(PROCESSED_DIR)\n",
    "         # os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "    else:\n",
    "         os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "         \n",
    "    emodb_metadata = process_dataset(EMODB_DIR, \"emodb\", PROCESSED_DIR)\n",
    "    ravdess_metadata = process_dataset(RAVDESS_DIR, \"ravdess\", PROCESSED_DIR)\n",
    "    \n",
    "    all_files_metadata = emodb_metadata + ravdess_metadata\n",
    "\n",
    "    if all_files_metadata: # Only save if processing was successful\n",
    "        # Save metadata list\n",
    "        np.save(metadata_path, all_files_metadata)\n",
    "        print(f\"\\nTotal processed files: {len(all_files_metadata)}\")\n",
    "        print(f\"Saved metadata to {metadata_path}\")\n",
    "    else:\n",
    "        print(\"\\nError: No files were processed successfully. Metadata not saved.\")\n",
    "else:\n",
    "    print(f\"\\nProcessed metadata file found. Loading metadata from {metadata_path}.\")\n",
    "    try:\n",
    "        all_files_metadata = np.load(metadata_path, allow_pickle=True).tolist()\n",
    "        print(f\"Loaded {len(all_files_metadata)} metadata entries.\")\n",
    "        if not all_files_metadata:\n",
    "            print(\"Warning: Loaded metadata is empty. Consider deleting the metadata file and re-running processing.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading metadata file: {e}. Please delete the file and re-run processing.\")\n",
    "        all_files_metadata = []\n",
    "\n",
    "# --- Verify Data --- \n",
    "if all_files_metadata:\n",
    "    print(\"\\nExample metadata entry:\", all_files_metadata[0])\n",
    "    # Load one spectrogram to check shape\n",
    "    try:\n",
    "        example_spec_path = all_files_metadata[0][0]\n",
    "        if os.path.exists(example_spec_path):\n",
    "             example_spec = np.load(example_spec_path)\n",
    "             print(\"Example spectrogram shape:\", example_spec.shape)\n",
    "             if example_spec.shape != (N_MELS, EXPECTED_TIME_STEPS):\n",
    "                 print(f\"ERROR: Loaded spectrogram shape {example_spec.shape} does not match expected shape {(N_MELS, EXPECTED_TIME_STEPS)}\")\n",
    "                 print(\"Please check the pad_truncate_spectrogram function and EXPECTED_TIME_STEPS.\")\n",
    "        else:\n",
    "             print(f\"Error: Example spectrogram file not found at {example_spec_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading example spectrogram: {e}\")\n",
    "else:\n",
    "    print(\"\\nMetadata is empty. Cannot proceed with verification or training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6. Create PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "pytorch-dataset"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset initialized with 1248 samples.\n",
      "Classes found: [0 1 2 3 4 5] -> 6 classes\n",
      "Number of classes detected in dataset: 6\n"
     ]
    }
   ],
   "source": [
    "class MelSpectrogramDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for loading mel spectrograms and labels.\"\"\"\n",
    "    def __init__(self, metadata, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            metadata (list): List of tuples (spectrogram_path, label_int).\n",
    "            transform (callable, optional): Optional transform to be applied \n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        # Filter out metadata entries where the file might not exist\n",
    "        self.metadata = [(p, l) for p, l in metadata if os.path.exists(p)]\n",
    "        if len(self.metadata) != len(metadata):\n",
    "             print(f\"Warning: Filtered out {len(metadata) - len(self.metadata)} non-existent file paths from metadata.\")\n",
    "             \n",
    "        self.transform = transform\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        # Fit label encoder to the unique labels present in metadata\n",
    "        all_labels = [item[1] for item in self.metadata]\n",
    "        if not all_labels:\n",
    "            print(\"Warning: No valid labels found in metadata for dataset creation.\")\n",
    "            self.num_classes = 0\n",
    "        else:\n",
    "            self.label_encoder.fit(sorted(list(set(all_labels))))\n",
    "            self.num_classes = len(self.label_encoder.classes_)\n",
    "            print(f\"Dataset initialized with {len(self.metadata)} samples.\")\n",
    "            print(f\"Classes found: {self.label_encoder.classes_} -> {self.num_classes} classes\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        # Handle potential index out of bounds if metadata was empty\n",
    "        if idx >= len(self.metadata):\n",
    "             raise IndexError(\"Index out of bounds for metadata list.\")\n",
    "\n",
    "        mel_path, label_int = self.metadata[idx]\n",
    "\n",
    "        try:\n",
    "            # Load spectrogram\n",
    "            mel_spec = np.load(mel_path) # Should be float32 already\n",
    "\n",
    "            # Add channel dimension (C, H, W) -> (1, N_MELS, TIME_STEPS)\n",
    "            mel_spec = np.expand_dims(mel_spec, axis=0)\n",
    "\n",
    "            # Convert label to tensor\n",
    "            label = torch.tensor(label_int, dtype=torch.long)\n",
    "\n",
    "            # Convert spectrogram to tensor\n",
    "            mel_spec_tensor = torch.from_numpy(mel_spec)\n",
    "\n",
    "            sample = {'spectrogram': mel_spec_tensor, 'label': label}\n",
    "\n",
    "            if self.transform:\n",
    "                sample = self.transform(sample)\n",
    "\n",
    "            # Return spectrogram, label, and index (useful for diffusion embedding lookup)\n",
    "            return sample['spectrogram'], sample['label'], idx\n",
    "\n",
    "        except FileNotFoundError:\n",
    "             print(f\"Error: File not found at {mel_path} for index {idx}. Check metadata consistency.\")\n",
    "             # Handle error: return dummy data, skip, or raise\n",
    "             # Raising might be best during development\n",
    "             raise FileNotFoundError(f\"File not found: {mel_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading or processing item at index {idx} ({mel_path}): {e}\")\n",
    "            # Raise the exception to stop execution and identify the problematic file\n",
    "            raise e\n",
    "\n",
    "# Create the full dataset\n",
    "full_dataset = None\n",
    "if all_files_metadata:\n",
    "    full_dataset = MelSpectrogramDataset(all_files_metadata)\n",
    "    if full_dataset.num_classes > 0:\n",
    "        NUM_EMOTIONS = full_dataset.num_classes # Update NUM_EMOTIONS based on actual data\n",
    "        print(f\"Number of classes detected in dataset: {NUM_EMOTIONS}\")\n",
    "    else:\n",
    "        print(\"Error: Dataset created but no classes found. Check metadata and label parsing.\")\n",
    "        full_dataset = None # Mark as unusable\n",
    "else:\n",
    "    print(\"Cannot create dataset, metadata is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7. Split Data into Training and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "data-split"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset initialized with 998 samples.\n",
      "Classes found: [0 1 2 3 4 5] -> 6 classes\n",
      "Dataset initialized with 250 samples.\n",
      "Classes found: [0 1 2 3 4 5] -> 6 classes\n",
      "Training set size: 998\n",
      "Validation set size: 250\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader = None, None\n",
    "train_dataset, val_dataset = None, None\n",
    "\n",
    "if full_dataset and len(full_dataset) > 0:\n",
    "    # Split metadata first to keep track of original files if needed\n",
    "    try:\n",
    "        train_meta, val_meta = train_test_split(\n",
    "            full_dataset.metadata, # Use the filtered metadata from the dataset object\n",
    "            test_size=0.2, # 20% for validation\n",
    "            random_state=SEED,\n",
    "            stratify=[item[1] for item in full_dataset.metadata] # Stratify by label\n",
    "        )\n",
    "\n",
    "        train_dataset = MelSpectrogramDataset(train_meta)\n",
    "        val_dataset = MelSpectrogramDataset(val_meta)\n",
    "\n",
    "        print(f\"Training set size: {len(train_dataset)}\")\n",
    "        print(f\"Validation set size: {len(val_dataset)}\")\n",
    "\n",
    "        # Create DataLoaders\n",
    "        # Use num_workers=0 if you encounter issues, especially on Windows or with certain Colab setups\n",
    "        train_loader = DataLoader(train_dataset, batch_size=RESNET_BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True, drop_last=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=RESNET_BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "        \n",
    "    except ValueError as e:\n",
    "        print(f\"Error during train/val split: {e}\")\n",
    "        print(\"This might happen if a class has too few samples for stratification.\")\n",
    "        print(\"Consider using a smaller validation split or checking data balance.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during data splitting: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"Dataset not created or is empty. Skipping data splitting and loader creation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Emotion Embedding Model (ResNet-50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "resnet-definition"
   },
   "outputs": [],
   "source": [
    "class EmotionResNet50(nn.Module):\n",
    "    \"\"\"ResNet-50 model adapted for 1-channel spectrogram input and emotion classification.\"\"\"\n",
    "    def __init__(self, num_classes, pretrained=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        weights = models.ResNet50_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "        self.resnet = models.resnet50(weights=weights)\n",
    "\n",
    "        # Modify the first convolutional layer to accept 1 input channel\n",
    "        original_conv1 = self.resnet.conv1\n",
    "        self.resnet.conv1 = nn.Conv2d(\n",
    "            1, original_conv1.out_channels,\n",
    "            kernel_size=original_conv1.kernel_size,\n",
    "            stride=original_conv1.stride,\n",
    "            padding=original_conv1.padding,\n",
    "            bias=(original_conv1.bias is not None)\n",
    "        )\n",
    "        \n",
    "        # Initialize the new conv1 weights \n",
    "        if pretrained:\n",
    "            # Average the weights of the original 3 channels to initialize the new 1 channel\n",
    "            original_weights = original_conv1.weight.data\n",
    "            mean_weights = torch.mean(original_weights, dim=1, keepdim=True)\n",
    "            self.resnet.conv1.weight.data = mean_weights\n",
    "            print(\"Initialized 1-channel conv1 weights by averaging pretrained weights.\")\n",
    "        else:\n",
    "             # Standard Kaiming initialization if not pretrained\n",
    "             nn.init.kaiming_normal_(self.resnet.conv1.weight, mode='fan_out', nonlinearity='relu')\n",
    "             print(\"Initialized 1-channel conv1 weights using Kaiming normal.\")\n",
    "        \n",
    "        if self.resnet.conv1.bias is not None:\n",
    "             nn.init.constant_(self.resnet.conv1.bias, 0)\n",
    "\n",
    "        # Modify the final fully connected layer for the desired number of emotion classes\n",
    "        num_ftrs = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Initialize the new fc layer\n",
    "        nn.init.xavier_uniform_(self.resnet.fc.weight)\n",
    "        if self.resnet.fc.bias is not None:\n",
    "            nn.init.constant_(self.resnet.fc.bias, 0)\n",
    "\n",
    "        print(f\"ResNet-50 adapted for {num_classes} emotion classes and 1 input channel.\")\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input x shape: (B, 1, H, W) e.g., (B, 1, 80, 862)\n",
    "        # ResNet expects H, W >= 32. Our N_MELS (H) is likely ok, Time (W) is large.\n",
    "        # We might need adaptive pooling if the fixed output size of ResNet layers causes issues\n",
    "        # before the final FC layer, but standard ResNet50 already includes AdaptiveAvgPool2d.\n",
    "        return self.resnet(x)\n",
    "\n",
    "    def get_embedding(self, x):\n",
    "        \"\"\"Extracts features before the final classification layer.\"\"\"\n",
    "        # Pass input through the ResNet layers up to the average pooling layer\n",
    "        x = self.resnet.conv1(x)\n",
    "        x = self.resnet.bn1(x)\n",
    "        x = self.resnet.relu(x)\n",
    "        x = self.resnet.maxpool(x)\n",
    "\n",
    "        x = self.resnet.layer1(x)\n",
    "        x = self.resnet.layer2(x)\n",
    "        x = self.resnet.layer3(x)\n",
    "        x = self.resnet.layer4(x)\n",
    "\n",
    "        # Use the adaptive average pooling layer defined in the original ResNet\n",
    "        x = self.resnet.avgpool(x)\n",
    "        embedding = torch.flatten(x, 1) # Flatten the output of avgpool\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "resnet-train-func"
   },
   "outputs": [],
   "source": [
    "def train_resnet_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, model_save_path):\n",
    "    \"\"\"Trains the ResNet-50 model.\"\"\"\n",
    "    best_val_accuracy = 0.0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "\n",
    "    print(f\"Starting ResNet training for {num_epochs} epochs on {device}...\")\n",
    "\n",
    "    # Check if loaders are valid\n",
    "    if not train_loader or not val_loader:\n",
    "        print(\"Error: Invalid data loaders provided.\")\n",
    "        return None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # --- Training Phase ---\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_predictions_train = 0\n",
    "        total_samples_train = 0\n",
    "\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "        for inputs, labels, _ in train_pbar: # Dataset returns inputs, labels, index\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_samples_train += labels.size(0)\n",
    "            correct_predictions_train += (predicted == labels).sum().item()\n",
    "\n",
    "            train_pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "            \n",
    "        # Check for zero division error\n",
    "        if total_samples_train == 0:\n",
    "             print(f\"Epoch {epoch+1} Warning: No training samples processed. Skipping epoch stats.\")\n",
    "             continue\n",
    "\n",
    "        epoch_loss_train = running_loss / total_samples_train\n",
    "        epoch_acc_train = correct_predictions_train / total_samples_train\n",
    "        history['train_loss'].append(epoch_loss_train)\n",
    "        history['train_acc'].append(epoch_acc_train)\n",
    "\n",
    "        # --- Validation Phase ---\n",
    "        model.eval()\n",
    "        running_loss_val = 0.0\n",
    "        correct_predictions_val = 0\n",
    "        total_samples_val = 0\n",
    "\n",
    "        val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels, _ in val_pbar: # Dataset returns inputs, labels, index\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                running_loss_val += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_samples_val += labels.size(0)\n",
    "                correct_predictions_val += (predicted == labels).sum().item()\n",
    "                val_pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "        \n",
    "        # Check for zero division error\n",
    "        if total_samples_val == 0:\n",
    "             print(f\"Epoch {epoch+1} Warning: No validation samples processed. Skipping epoch stats.\")\n",
    "             epoch_loss_val = float('inf')\n",
    "             epoch_acc_val = 0.0\n",
    "        else:\n",
    "            epoch_loss_val = running_loss_val / total_samples_val\n",
    "            epoch_acc_val = correct_predictions_val / total_samples_val\n",
    "            \n",
    "        history['val_loss'].append(epoch_loss_val)\n",
    "        history['val_acc'].append(epoch_acc_val)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "              f\"Train Loss: {epoch_loss_train:.4f}, Train Acc: {epoch_acc_train:.4f} | \"\n",
    "              f\"Val Loss: {epoch_loss_val:.4f}, Val Acc: {epoch_acc_val:.4f}\")\n",
    "\n",
    "        # Save the model if validation accuracy improves\n",
    "        if epoch_acc_val > best_val_accuracy and total_samples_val > 0:\n",
    "            best_val_accuracy = epoch_acc_val\n",
    "            try:\n",
    "                torch.save(model.state_dict(), model_save_path)\n",
    "                print(f\"Best model saved to {model_save_path} (Val Acc: {best_val_accuracy:.4f})\")\n",
    "            except Exception as e:\n",
    "                 print(f\"Error saving model: {e}\")\n",
    "\n",
    "    print(\"Finished Training ResNet.\")\n",
    "    print(f\"Best Validation Accuracy: {best_val_accuracy:.4f}\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Train or Load ResNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "resnet-train-load"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to C:\\Users\\Youssef/.cache\\torch\\hub\\checkpoints\\resnet50-0676ba61.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 97.8M/97.8M [01:34<00:00, 1.09MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized 1-channel conv1 weights by averaging pretrained weights.\n",
      "ResNet-50 adapted for 6 emotion classes and 1 input channel.\n",
      "No pre-trained ResNet model found. Training from scratch...\n",
      "Starting ResNet training for 50 epochs on cpu...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c3f6108417c4ce49e122e272beec839",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/50 [Train]:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Youssef\\Documents\\4eme2425\\Deeplearning\\Emotion reconigtion project\\my_audio_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 19596, 16364) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32m~\\Documents\\4eme2425\\Deeplearning\\Emotion reconigtion project\\my_audio_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1284\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1283\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1284\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\multiprocessing\\queues.py:114\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout):\n\u001b[1;32m--> 114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo pre-trained ResNet model found. Training from scratch...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train_loader \u001b[38;5;129;01mand\u001b[39;00m val_loader:\n\u001b[1;32m---> 27\u001b[0m     history_resnet \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_resnet_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresnet_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion_resnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_resnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mRESNET_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mRESNET_MODEL_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot train ResNet model, data loaders not available.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[12], line 21\u001b[0m, in \u001b[0;36mtrain_resnet_model\u001b[1;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, model_save_path)\u001b[0m\n\u001b[0;32m     18\u001b[0m total_samples_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     20\u001b[0m train_pbar \u001b[38;5;241m=\u001b[39m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m [Train]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels, _ \u001b[38;5;129;01min\u001b[39;00m train_pbar: \u001b[38;5;66;03m# Dataset returns inputs, labels, index\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# Zero the parameter gradients\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\4eme2425\\Deeplearning\\Emotion reconigtion project\\my_audio_env\\lib\\site-packages\\tqdm\\notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[1;32m--> 250\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[0;32m    251\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\4eme2425\\Deeplearning\\Emotion reconigtion project\\my_audio_env\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\4eme2425\\Deeplearning\\Emotion reconigtion project\\my_audio_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    739\u001b[0m ):\n",
      "File \u001b[1;32m~\\Documents\\4eme2425\\Deeplearning\\Emotion reconigtion project\\my_audio_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1491\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data, worker_id)\n\u001b[0;32m   1490\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1491\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1492\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1494\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\4eme2425\\Deeplearning\\Emotion reconigtion project\\my_audio_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1453\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1449\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1450\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1451\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1452\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1453\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1454\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1455\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32m~\\Documents\\4eme2425\\Deeplearning\\Emotion reconigtion project\\my_audio_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1297\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1296\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[1;32m-> 1297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1298\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1299\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[0;32m   1301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 19596, 16364) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "# Initialize model, criterion, optimizer\n",
    "resnet_model = None\n",
    "history_resnet = None\n",
    "if NUM_EMOTIONS > 0:\n",
    "    resnet_model = EmotionResNet50(num_classes=NUM_EMOTIONS, pretrained=True).to(DEVICE)\n",
    "    criterion_resnet = nn.CrossEntropyLoss()\n",
    "    optimizer_resnet = optim.Adam(resnet_model.parameters(), lr=RESNET_LEARNING_RATE)\n",
    "\n",
    "    # Check if a trained model exists\n",
    "    if os.path.exists(RESNET_MODEL_PATH):\n",
    "        print(f\"Loading pre-trained ResNet model from {RESNET_MODEL_PATH}...\")\n",
    "        try:\n",
    "            resnet_model.load_state_dict(torch.load(RESNET_MODEL_PATH, map_location=DEVICE))\n",
    "            print(\"ResNet model loaded successfully.\")\n",
    "            # Optionally run evaluation on val set to confirm performance\n",
    "            # evaluate_model(resnet_model, val_loader, criterion_resnet, DEVICE, NUM_EMOTIONS) # Need evaluate_model defined\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading ResNet model: {e}. Training from scratch.\")\n",
    "            if train_loader and val_loader:\n",
    "                 history_resnet = train_resnet_model(resnet_model, train_loader, val_loader, criterion_resnet, optimizer_resnet, RESNET_EPOCHS, DEVICE, RESNET_MODEL_PATH)\n",
    "            else:\n",
    "                 print(\"Cannot train ResNet model, data loaders not available.\")\n",
    "                 resnet_model = None # Mark as None if training cannot proceed\n",
    "    else:\n",
    "        print(\"No pre-trained ResNet model found. Training from scratch...\")\n",
    "        if train_loader and val_loader:\n",
    "            history_resnet = train_resnet_model(resnet_model, train_loader, val_loader, criterion_resnet, optimizer_resnet, RESNET_EPOCHS, DEVICE, RESNET_MODEL_PATH)\n",
    "        else:\n",
    "            print(\"Cannot train ResNet model, data loaders not available.\")\n",
    "            resnet_model = None # Mark as None if training cannot proceed\n",
    "else:\n",
    "    print(\"Error: Number of emotion classes is zero. Cannot initialize or train ResNet model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Function to Extract Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "extract-embeddings-func"
   },
   "outputs": [],
   "source": [
    "def extract_embeddings(model, data_loader, device):\n",
    "    \"\"\"Extracts embeddings from the ResNet model for all data in the loader.\"\"\"\n",
    "    if model is None or data_loader is None:\n",
    "        print(\"Error: Model or data loader is None. Cannot extract embeddings.\")\n",
    "        return None, None, None\n",
    "        \n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    labels_list = []\n",
    "    indices_list = [] # Store indices to ensure correct mapping\n",
    "    print(f\"Extracting embeddings using device: {device}\")\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, indices in tqdm(data_loader, desc=\"Extracting Embeddings\"):\n",
    "            inputs = inputs.to(device)\n",
    "            # Get embeddings from the model\n",
    "            batch_embeddings = model.get_embedding(inputs)\n",
    "            embeddings.append(batch_embeddings.cpu().numpy())\n",
    "            labels_list.append(labels.cpu().numpy())\n",
    "            indices_list.append(indices.cpu().numpy()) # Store batch indices\n",
    "            \n",
    "    if not embeddings:\n",
    "        print(\"Warning: No embeddings were extracted.\")\n",
    "        return None, None, None\n",
    "\n",
    "    embeddings = np.concatenate(embeddings, axis=0)\n",
    "    labels_list = np.concatenate(labels_list, axis=0)\n",
    "    indices_list = np.concatenate(indices_list, axis=0)\n",
    "    print(f\"Extracted embeddings shape: {embeddings.shape}\")\n",
    "    \n",
    "    # Sort embeddings based on indices to match the original dataset order\n",
    "    sort_order = np.argsort(indices_list)\n",
    "    embeddings_sorted = embeddings[sort_order]\n",
    "    labels_sorted = labels_list[sort_order]\n",
    "    indices_sorted = indices_list[sort_order]\n",
    "    \n",
    "    # Verify sorting\n",
    "    if not np.all(indices_sorted == np.arange(len(indices_sorted))):\n",
    "         print(\"Warning: Index sorting verification failed. Embeddings might not match dataset order.\")\n",
    "    \n",
    "    return embeddings_sorted, labels_sorted, indices_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5. Extract Embeddings for Conditioning Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "extract-embeddings-exec"
   },
   "outputs": [],
   "source": [
    "# Extract embeddings for the full dataset (needed for conditioning diffusion model)\n",
    "all_embeddings, all_labels, all_indices = None, None, None\n",
    "EMBEDDING_DIM = 2048 # Default for ResNet50 avgpool output\n",
    "\n",
    "if full_dataset and resnet_model:\n",
    "    # Create a loader for the full dataset \n",
    "    # Use num_workers=0 if issues arise\n",
    "    full_loader = DataLoader(full_dataset, batch_size=RESNET_BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    print(\"Extracting embeddings for the full dataset...\")\n",
    "    all_embeddings, all_labels, all_indices = extract_embeddings(resnet_model, full_loader, DEVICE)\n",
    "    \n",
    "    if all_embeddings is not None:\n",
    "        EMBEDDING_DIM = all_embeddings.shape[1] # Get actual embedding dimension\n",
    "        print(f\"Embedding dimension: {EMBEDDING_DIM}\")\n",
    "        # Create a mapping from label index back to emotion name for clarity\n",
    "        label_to_emotion = {i: name for name, i in EMOTIONS.items()}\n",
    "        print(\"Embeddings extracted successfully.\")\n",
    "    else:\n",
    "        print(\"Embedding extraction failed.\")\n",
    "        EMBEDDING_DIM = 2048 # Fallback to default if extraction failed\n",
    "else:\n",
    "    print(\"Full dataset or ResNet model not available, cannot extract embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Diffusion Model for Mel-Spectrogram Generation\n",
    "\n",
    "Implementing a full diffusion model from scratch is complex. This section provides a structure based on the paper's description (U-Net, ResNet blocks, time/emotion conditioning). It may require further refinement and tuning for optimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Diffusion Model Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "diffusion-components"
   },
   "outputs": [],
   "source": [
    "# --- Helper: Sinusoidal Time Embeddings ---\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = np.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :] # Shape: (batch_size, half_dim)\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1) # Shape: (batch_size, dim)\n",
    "        return emb\n",
    "\n",
    "# --- Helper: ResNet Block with Time/Emotion Conditioning ---\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, time_emb_dim=None, emotion_emb_dim=None, use_attention=False):\n",
    "        super().__init__()\n",
    "        self.use_attention = use_attention\n",
    "        groups = 8 # GroupNorm groups, adjust if needed\n",
    "\n",
    "        # Time embedding projection\n",
    "        self.time_mlp = None\n",
    "        if time_emb_dim is not None:\n",
    "            self.time_mlp = nn.Sequential(\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(time_emb_dim, out_channels * 2) # Project to scale and shift\n",
    "            )\n",
    "            \n",
    "        # Emotion embedding projection\n",
    "        self.emotion_mlp = None\n",
    "        if emotion_emb_dim is not None:\n",
    "             self.emotion_mlp = nn.Sequential(\n",
    "                 nn.SiLU(),\n",
    "                 nn.Linear(emotion_emb_dim, out_channels * 2) # Project to scale and shift\n",
    "             )\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        # Ensure out_channels is divisible by groups for GroupNorm\n",
    "        self.norm1 = nn.GroupNorm(max(1, groups // (in_channels // out_channels)) if out_channels < groups else groups, out_channels)\n",
    "        self.act1 = nn.SiLU() # Swish activation\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.norm2 = nn.GroupNorm(groups, out_channels)\n",
    "        self.act2 = nn.SiLU()\n",
    "\n",
    "        # Residual connection matching\n",
    "        self.residual_conv = nn.Conv2d(in_channels, out_channels, 1) if in_channels != out_channels else nn.Identity()\n",
    "\n",
    "        # Optional: Basic Linear Attention (simplified)\n",
    "        self.attention = None\n",
    "        if use_attention:\n",
    "             # Simplified self-attention like mechanism (Conv based)\n",
    "             attn_hidden_dim = max(16, out_channels // 8) # Ensure reasonable hidden dim\n",
    "             self.attention = nn.Sequential(\n",
    "                 nn.Conv2d(out_channels, attn_hidden_dim, 1),\n",
    "                 nn.SiLU(),\n",
    "                 nn.Conv2d(attn_hidden_dim, out_channels, 1),\n",
    "                 nn.Sigmoid() # Scale features based on attention map\n",
    "             )\n",
    "\n",
    "    def forward(self, x, t_emb=None, e_emb=None):\n",
    "        res = x\n",
    "\n",
    "        # First convolution block\n",
    "        h = self.conv1(x)\n",
    "        h = self.norm1(h)\n",
    "\n",
    "        # Apply time embedding conditioning (FiLM-like)\n",
    "        if self.time_mlp is not None and t_emb is not None:\n",
    "             time_cond = self.time_mlp(t_emb)\n",
    "             time_cond = time_cond.unsqueeze(-1).unsqueeze(-1) # Reshape (B, C*2, 1, 1)\n",
    "             scale_time, shift_time = time_cond.chunk(2, dim=1) \n",
    "             h = h * (scale_time + 1) + shift_time \n",
    "\n",
    "        # Apply emotion embedding conditioning (FiLM-like)\n",
    "        if self.emotion_mlp is not None and e_emb is not None:\n",
    "             emotion_cond = self.emotion_mlp(e_emb)\n",
    "             emotion_cond = emotion_cond.unsqueeze(-1).unsqueeze(-1)\n",
    "             scale_emotion, shift_emotion = emotion_cond.chunk(2, dim=1)\n",
    "             h = h * (scale_emotion + 1) + shift_emotion\n",
    "             \n",
    "        h = self.act1(h)\n",
    "\n",
    "        # Second convolution block\n",
    "        h = self.conv2(h)\n",
    "        h = self.norm2(h)\n",
    "        h = self.act2(h)\n",
    "\n",
    "        # Apply attention if enabled\n",
    "        if self.attention is not None:\n",
    "            attn_map = self.attention(h)\n",
    "            h = h * attn_map\n",
    "\n",
    "        # Add residual connection\n",
    "        return h + self.residual_conv(res)\n",
    "\n",
    "# --- Helper: Downsample/Upsample ---\n",
    "class Downsample(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        # Use AvgPool2d for downsampling feature maps might be smoother\n",
    "        # self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        # Or keep Conv stride 2\n",
    "        self.conv = nn.Conv2d(channels, channels, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # return self.pool(x)\n",
    "        return self.conv(x)\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        # Use bilinear interpolation + conv for potentially better quality\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        self.conv = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.upsample(x)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "# --- U-Net Architecture ---\n",
    "class UNetDiffusion(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=1,              # Input spectrogram channels\n",
    "        model_channels=64,          # Base channel count\n",
    "        out_channels=1,             # Output channels (usually same as input)\n",
    "        channel_mult=(1, 2, 4, 8),  # Channel multipliers per level\n",
    "        num_res_blocks=2,           # ResBlocks per level\n",
    "        time_emb_dim=256,           # Dimension for time embedding\n",
    "        emotion_emb_dim=None,       # Dimension for emotion embedding (e.g., 2048 from ResNet)\n",
    "        use_attention_levels=(False, False, True, True), # Apply attention at deeper levels\n",
    "        dropout=0.1                 # Dropout rate\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if emotion_emb_dim is None:\n",
    "            print(\"Warning: emotion_emb_dim not provided to UNetDiffusion. Emotion conditioning disabled.\")\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.model_channels = model_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.channel_mult = channel_mult\n",
    "        self.num_res_blocks = num_res_blocks\n",
    "        self.time_emb_dim = time_emb_dim\n",
    "        self.emotion_emb_dim = emotion_emb_dim\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # --- Time Embedding ---\n",
    "        time_dim = model_channels * 4 # Internal dimension for time projection\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPosEmb(model_channels),\n",
    "            nn.Linear(model_channels, time_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_dim, time_dim)\n",
    "        )\n",
    "\n",
    "        # --- Initial Convolution ---\n",
    "        self.init_conv = nn.Conv2d(in_channels, model_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        # --- Downsampling Path ---\n",
    "        self.down_blocks = nn.ModuleList()\n",
    "        channels = [model_channels]\n",
    "        now_channels = model_channels\n",
    "        for i, mult in enumerate(channel_mult):\n",
    "            out_channels_level = model_channels * mult\n",
    "            use_attn = use_attention_levels[i]\n",
    "            for _ in range(num_res_blocks):\n",
    "                self.down_blocks.append(ResidualBlock(\n",
    "                    now_channels, out_channels_level, time_dim, emotion_emb_dim, use_attn\n",
    "                ))\n",
    "                now_channels = out_channels_level\n",
    "                channels.append(now_channels)\n",
    "            if i != len(channel_mult) - 1: # Don't downsample at the last level\n",
    "                self.down_blocks.append(Downsample(now_channels))\n",
    "                channels.append(now_channels)\n",
    "\n",
    "        # --- Bottleneck ---\n",
    "        self.mid_block1 = ResidualBlock(now_channels, now_channels, time_dim, emotion_emb_dim, use_attention=True)\n",
    "        # Add dropout maybe?\n",
    "        # self.mid_attn = AttentionBlock(now_channels)\n",
    "        self.mid_block2 = ResidualBlock(now_channels, now_channels, time_dim, emotion_emb_dim, use_attention=False)\n",
    "\n",
    "        # --- Upsampling Path ---\n",
    "        self.up_blocks = nn.ModuleList()\n",
    "        for i, mult in reversed(list(enumerate(channel_mult))):\n",
    "            out_channels_level = model_channels * mult\n",
    "            use_attn = use_attention_levels[i]\n",
    "            for _ in range(num_res_blocks + 1): # +1 to handle skip connection merging\n",
    "                skip_channels_in = channels.pop()\n",
    "                self.up_blocks.append(ResidualBlock(\n",
    "                    now_channels + skip_channels_in, # Input channels = current + skip\n",
    "                    out_channels_level,\n",
    "                    time_dim,\n",
    "                    emotion_emb_dim,\n",
    "                    use_attn\n",
    "                ))\n",
    "                now_channels = out_channels_level\n",
    "            if i != 0: # Don't upsample after the first level (closest to output)\n",
    "                self.up_blocks.append(Upsample(now_channels))\n",
    "\n",
    "        # --- Final Layers ---\n",
    "        final_groups = 8 # GroupNorm groups for final layer\n",
    "        self.final_norm = nn.GroupNorm(max(1, final_groups // (in_channels // model_channels)) if model_channels < final_groups else final_groups, model_channels)\n",
    "        self.final_act = nn.SiLU()\n",
    "        self.final_conv = nn.Conv2d(model_channels, self.out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, time, emotion_embedding=None):\n",
    "        # x: Input spectrogram (B, C, H, W) -> (B, 1, N_MELS, TIME)\n",
    "        # time: Timestep tensor (B,)\n",
    "        # emotion_embedding: Emotion embedding tensor (B, emotion_emb_dim)\n",
    "\n",
    "        # 1. Time Embedding\n",
    "        t_emb = self.time_mlp(time) # (B, time_dim)\n",
    "\n",
    "        # 2. Initial Convolution\n",
    "        h = self.init_conv(x) # (B, model_channels, H, W)\n",
    "        skips = [h] # Store initial skip connection\n",
    "\n",
    "        # 3. Downsampling Path\n",
    "        for block in self.down_blocks:\n",
    "            if isinstance(block, ResidualBlock):\n",
    "                 h = block(h, t_emb, emotion_embedding)\n",
    "                 skips.append(h)\n",
    "            elif isinstance(block, Downsample):\n",
    "                 h = block(h)\n",
    "                 # Skip connection comes from *before* downsampling in standard UNets\n",
    "                 # The current logic stores skip *after* resblock and *after* downsample.\n",
    "                 # Let's stick to storing after resblock for now.\n",
    "                 # skips.append(h) # Standard UNet would skip before downsampling\n",
    "        \n",
    "        # 4. Bottleneck\n",
    "        h = self.mid_block1(h, t_emb, emotion_embedding)\n",
    "        h = self.mid_block2(h, t_emb, emotion_embedding)\n",
    "\n",
    "        # 5. Upsampling Path\n",
    "        for block in self.up_blocks:\n",
    "            if isinstance(block, ResidualBlock):\n",
    "                # Pop the corresponding skip connection\n",
    "                skip = skips.pop()\n",
    "                # Concatenate along channel dim\n",
    "                # Ensure spatial dimensions match before concat\n",
    "                if h.shape[-2:] != skip.shape[-2:]:\n",
    "                     # Simple interpolation if shapes differ slightly\n",
    "                     h = F.interpolate(h, size=skip.shape[-2:], mode='bilinear', align_corners=False)\n",
    "                h = torch.cat([h, skip], dim=1) \n",
    "                h = block(h, t_emb, emotion_embedding)\n",
    "            elif isinstance(block, Upsample):\n",
    "                h = block(h)\n",
    "\n",
    "        # 6. Final Layers\n",
    "        h = self.final_norm(h)\n",
    "        h = self.final_act(h)\n",
    "        h = self.final_conv(h) # (B, out_channels, H, W)\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Diffusion Process (DDPM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ddpm-math"
   },
   "outputs": [],
   "source": [
    "def linear_beta_schedule(timesteps):\n",
    "    \"\"\"Linear schedule from _start=0.0001 to _end=0.02.\"\"\"\n",
    "    scale = 1000 / timesteps\n",
    "    beta_start = scale * 0.0001\n",
    "    beta_end = scale * 0.02\n",
    "    return torch.linspace(beta_start, beta_end, timesteps, dtype=torch.float64)\n",
    "\n",
    "# --- Precompute DDPM variables ---\n",
    "T = DIFFUSION_TIMESTEPS\n",
    "# Ensure calculations happen on the correct device from the start if possible\n",
    "# Or move tensors to device later when needed inside functions\n",
    "betas = linear_beta_schedule(T).to(dtype=torch.float32) # Use float32 for model compatibility\n",
    "alphas = 1. - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "\n",
    "# Calculations for diffusion q(x_t | x_0)\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n",
    "\n",
    "# Calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n",
    "# Clamp variance to avoid issues at t=0 where variance is theoretically zero\n",
    "posterior_variance_clipped = torch.clamp(posterior_variance, min=1e-20)\n",
    "posterior_log_variance_clipped = torch.log(posterior_variance_clipped)\n",
    "\n",
    "# Coefficients for posterior mean calculation (used in p_sample)\n",
    "posterior_mean_coef1 = betas * torch.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod)\n",
    "posterior_mean_coef2 = (1. - alphas_cumprod_prev) * torch.sqrt(alphas) / (1. - alphas_cumprod)\n",
    "\n",
    "# Move precomputed tensors to the target device\n",
    "betas = betas.to(DEVICE)\n",
    "alphas = alphas.to(DEVICE)\n",
    "alphas_cumprod = alphas_cumprod.to(DEVICE)\n",
    "alphas_cumprod_prev = alphas_cumprod_prev.to(DEVICE)\n",
    "sqrt_alphas_cumprod = sqrt_alphas_cumprod.to(DEVICE)\n",
    "sqrt_one_minus_alphas_cumprod = sqrt_one_minus_alphas_cumprod.to(DEVICE)\n",
    "posterior_variance_clipped = posterior_variance_clipped.to(DEVICE)\n",
    "posterior_log_variance_clipped = posterior_log_variance_clipped.to(DEVICE)\n",
    "posterior_mean_coef1 = posterior_mean_coef1.to(DEVICE)\n",
    "posterior_mean_coef2 = posterior_mean_coef2.to(DEVICE)\n",
    "\n",
    "\n",
    "# --- Helper function to extract alpha/beta values for a batch of timesteps ---\n",
    "def extract(a, t, x_shape):\n",
    "    batch_size = t.shape[0]\n",
    "    # Gather based on timestep indices t. Ensure t is on CPU for gather if a is on CPU initially.\n",
    "    # However, we moved 'a' to DEVICE, so t should also be on DEVICE.\n",
    "    out = a.gather(-1, t) \n",
    "    # Reshape to broadcast correctly: (batch_size, 1, 1, 1) for image/spectrogram data\n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1)))\n",
    "\n",
    "# --- Forward Diffusion (Noise addition) ---\n",
    "def q_sample(x_start, t, noise=None):\n",
    "    \"\"\"Applies noise to x_start for timesteps t.\"\"\"\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x_start)\n",
    "\n",
    "    sqrt_alphas_cumprod_t = extract(sqrt_alphas_cumprod, t, x_start.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = extract(sqrt_one_minus_alphas_cumprod, t, x_start.shape)\n",
    "\n",
    "    return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "\n",
    "# --- Denoising Loss Calculation ---\n",
    "def p_losses(denoise_model, x_start, t, emotion_embeddings, noise=None, loss_type=\"l2\"):\n",
    "    \"\"\"Calculates the loss for the denoising model.\"\"\"\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x_start)\n",
    "\n",
    "    # Apply noise to the original data\n",
    "    x_noisy = q_sample(x_start=x_start, t=t, noise=noise)\n",
    "    \n",
    "    # Get the model's prediction for the noise\n",
    "    predicted_noise = denoise_model(x_noisy, t, emotion_embeddings) \n",
    "\n",
    "    # Calculate the loss between the actual noise and predicted noise\n",
    "    if loss_type == 'l1':\n",
    "        loss = F.l1_loss(noise, predicted_noise)\n",
    "    elif loss_type == 'l2':\n",
    "        loss = F.mse_loss(noise, predicted_noise)\n",
    "    elif loss_type == \"huber\":\n",
    "        loss = F.smooth_l1_loss(noise, predicted_noise)\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Loss type '{loss_type}' not implemented.\")\n",
    "\n",
    "    return loss\n",
    "\n",
    "# --- Sampling Functions (Reverse Process) ---\n",
    "@torch.no_grad()\n",
    "def p_sample(model, x, t, t_index, emotion_embeddings):\n",
    "    \"\"\"Single step of the reverse diffusion process (Algorithm 2 in DDPM).\"\"\"\n",
    "    # Use coefficients derived for q(x_{t-1} | x_t, x_0)\n",
    "    betas_t = extract(betas, t, x.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = extract(sqrt_one_minus_alphas_cumprod, t, x.shape)\n",
    "    sqrt_recip_alphas_t = extract(torch.sqrt(1.0 / alphas), t, x.shape)\n",
    "    \n",
    "    # Predict the noise using the model\n",
    "    predicted_noise = model(x, t, emotion_embeddings)\n",
    "    \n",
    "    # Calculate the mean of the posterior distribution q(x_{t-1} | x_t, x_0)\n",
    "    # This uses the predicted noise to estimate x_0 and then plugs into the formula\n",
    "    # model_mean = posterior_mean_coef1_t * x_0_pred + posterior_mean_coef2_t * x_t\n",
    "    # A more direct form using predicted_noise (Equation 11 in DDPM paper):\n",
    "    model_mean = sqrt_recip_alphas_t * (\n",
    "        x - betas_t * predicted_noise / sqrt_one_minus_alphas_cumprod_t\n",
    "    )\n",
    "\n",
    "    if t_index == 0:\n",
    "        return model_mean # No noise added at the last step\n",
    "    else:\n",
    "        # Get the variance of the posterior distribution\n",
    "        posterior_variance_t = extract(posterior_variance_clipped, t, x.shape)\n",
    "        noise = torch.randn_like(x)\n",
    "        # Add noise scaled by the posterior variance\n",
    "        return model_mean + torch.sqrt(posterior_variance_t) * noise\n",
    "\n",
    "@torch.no_grad()\n",
    "def p_sample_loop(model, shape, timesteps, emotion_embeddings):\n",
    "    \"\"\"Full sampling loop from noise to data.\"\"\"\n",
    "    device = next(model.parameters()).device # Get device from model\n",
    "    b = shape[0] # Batch size from shape\n",
    "    \n",
    "    # Start from pure noise (x_T)\n",
    "    img = torch.randn(shape, device=device)\n",
    "    imgs = [] # Optional: Store intermediate images\n",
    "\n",
    "    # Iterate backwards through timesteps\n",
    "    for i in tqdm(reversed(range(0, timesteps)), desc='Sampling loop time step', total=timesteps):\n",
    "        # Create tensor for current timestep for all samples in batch\n",
    "        time_tensor = torch.full((b,), i, device=device, dtype=torch.long)\n",
    "        # Perform one denoising step\n",
    "        img = p_sample(model, img, time_tensor, i, emotion_embeddings)\n",
    "        # Optionally store intermediate results:\n",
    "        # if i % 50 == 0: imgs.append(img.cpu().numpy())\n",
    "\n",
    "    # imgs.append(img.cpu().numpy()) # Store final result\n",
    "    return img # Return the final denoised image x_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Diffusion Model Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "diffusion-train-loop"
   },
   "outputs": [],
   "source": [
    "class DiffusionDatasetWrapper(Dataset):\n",
    "    \"\"\"Wraps the MelSpectrogramDataset to return embeddings along with spectrograms.\"\"\"\n",
    "    def __init__(self, original_dataset, embeddings):\n",
    "        self.original_dataset = original_dataset\n",
    "        # Ensure embeddings are float tensors and match dataset length\n",
    "        if len(original_dataset) != len(embeddings):\n",
    "             raise ValueError(f\"Dataset length ({len(original_dataset)}) and embeddings length ({len(embeddings)}) mismatch.\")\n",
    "        self.embeddings = torch.from_numpy(embeddings).float() \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.original_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get spectrogram, original label, and original index from the underlying dataset\n",
    "        spectrogram, original_label, original_idx = self.original_dataset[idx]\n",
    "        \n",
    "        # Use the original index to fetch the pre-computed embedding\n",
    "        # This assumes 'all_embeddings' were extracted in the same order as the dataset items\n",
    "        if original_idx != idx:\n",
    "             # This check should ideally not fail if extraction was done with shuffle=False\n",
    "             print(f\"Warning: Dataset index {idx} does not match returned original index {original_idx}. Using idx for embedding lookup.\")\n",
    "             lookup_idx = idx\n",
    "        else:\n",
    "             lookup_idx = original_idx\n",
    "             \n",
    "        # Handle potential index out of bounds for embeddings\n",
    "        if lookup_idx >= len(self.embeddings):\n",
    "             raise IndexError(f\"Index {lookup_idx} out of bounds for embeddings list (length {len(self.embeddings)}). Check dataset/embedding alignment.\")\n",
    "             \n",
    "        embedding = self.embeddings[lookup_idx]\n",
    "        \n",
    "        return spectrogram, embedding # Return spectrogram and its corresponding embedding\n",
    "\n",
    "def train_diffusion_model(model, dataset, optimizer, num_epochs, batch_size, device, model_save_path, all_embeddings):\n",
    "    \"\"\"Trains the Diffusion model.\"\"\"\n",
    "    if model is None or dataset is None or all_embeddings is None:\n",
    "        print(\"Error: Model, dataset, or embeddings not provided. Cannot train diffusion model.\")\n",
    "        return None\n",
    "        \n",
    "    # Create the wrapped dataset for training\n",
    "    try:\n",
    "        diffusion_train_dataset = DiffusionDatasetWrapper(dataset, all_embeddings)\n",
    "        # Use num_workers=0 if issues arise\n",
    "        diffusion_loader = DataLoader(diffusion_train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True, drop_last=True)\n",
    "    except ValueError as e:\n",
    "         print(f\"Error creating diffusion dataset/loader: {e}\")\n",
    "         return None\n",
    "    except Exception as e:\n",
    "         print(f\"Unexpected error creating diffusion dataset/loader: {e}\")\n",
    "         return None\n",
    "\n",
    "    print(f\"Starting Diffusion Model training for {num_epochs} epochs on {device}...\")\n",
    "\n",
    "    model.to(device) # Ensure model is on the correct device\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(device == 'cuda')) # Automatic Mixed Precision\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        num_samples = 0\n",
    "\n",
    "        pbar = tqdm(diffusion_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Diffusion Train]\")\n",
    "        for step, (batch_spectrograms, batch_embeddings) in enumerate(pbar):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            batch_size_current = batch_spectrograms.shape[0]\n",
    "            batch_spectrograms = batch_spectrograms.to(device)\n",
    "            batch_embeddings = batch_embeddings.to(device) # Send embeddings to device\n",
    "\n",
    "            # Sample random timesteps for this batch\n",
    "            t = torch.randint(0, DIFFUSION_TIMESTEPS, (batch_size_current,), device=device).long()\n",
    "\n",
    "            # Calculate loss using mixed precision\n",
    "            with torch.cuda.amp.autocast(enabled=(device == 'cuda')):\n",
    "                 loss = p_losses(model, batch_spectrograms, t, batch_embeddings, loss_type=\"l2\") # Use MSE loss\n",
    "\n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                 print(f\"Warning: NaN or Inf loss encountered at epoch {epoch+1}, step {step}. Skipping batch.\")\n",
    "                 # Consider stopping training or reducing learning rate if this happens frequently\n",
    "                 continue\n",
    "\n",
    "            # Scales loss. Calls backward() on scaled loss to create scaled gradients.\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            # Optional: Gradient clipping\n",
    "            # scaler.unscale_(optimizer) # Unscales gradients before clipping\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            # scaler.step() first unscales the gradients of the optimizer's assigned params.\n",
    "            # If these gradients do not contain infs or NaNs, optimizer.step() is then called.\n",
    "            # Otherwise, optimizer.step() is skipped.\n",
    "            scaler.step(optimizer)\n",
    "\n",
    "            # Updates the scale for next iteration.\n",
    "            scaler.update()\n",
    "\n",
    "            running_loss += loss.item() * batch_size_current\n",
    "            num_samples += batch_size_current\n",
    "            pbar.set_postfix({'loss': loss.item():.6f})\n",
    "            \n",
    "        if num_samples == 0:\n",
    "             print(f\"Epoch {epoch+1} Warning: No samples processed. Skipping epoch stats.\")\n",
    "             continue\n",
    "             \n",
    "        epoch_loss = running_loss / num_samples\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Diffusion Loss: {epoch_loss:.6f}\")\n",
    "\n",
    "        # Save the model periodically\n",
    "        if (epoch + 1) % 10 == 0 or epoch == num_epochs - 1: # Save every 10 epochs and at the end\n",
    "            try:\n",
    "                torch.save(model.state_dict(), model_save_path)\n",
    "                print(f\"Diffusion model saved to {model_save_path} at epoch {epoch+1}\")\n",
    "            except Exception as e:\n",
    "                 print(f\"Error saving diffusion model: {e}\")\n",
    "\n",
    "    print(\"Finished Training Diffusion Model.\")\n",
    "    return model # Return trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4. Train or Load Diffusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "diffusion-train-load"
   },
   "outputs": [],
   "source": [
    "# Initialize Diffusion Model and Optimizer\n",
    "diffusion_model = None\n",
    "if EMBEDDING_DIM is not None and NUM_EMOTIONS > 0:\n",
    "    diffusion_model = UNetDiffusion(\n",
    "        in_channels=1,\n",
    "        model_channels=64, # Base channels - adjust complexity\n",
    "        out_channels=1,\n",
    "        channel_mult=(1, 2, 3, 4), # Deeper U-Net, e.g., (1, 2, 4, 8) might be better but slower\n",
    "        num_res_blocks=2,\n",
    "        time_emb_dim=256, # Should match time_dim in UNet definition if changed\n",
    "        emotion_emb_dim=EMBEDDING_DIM, # Use dimension from ResNet output\n",
    "        use_attention_levels=(False, False, True, True) # Use attention in deeper layers\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    optimizer_diffusion = optim.AdamW(diffusion_model.parameters(), lr=DIFFUSION_LEARNING_RATE, weight_decay=1e-4)\n",
    "\n",
    "    # Check if a trained diffusion model exists\n",
    "    if os.path.exists(DIFFUSION_MODEL_PATH):\n",
    "        print(f\"Loading pre-trained Diffusion model from {DIFFUSION_MODEL_PATH}...\")\n",
    "        try:\n",
    "            diffusion_model.load_state_dict(torch.load(DIFFUSION_MODEL_PATH, map_location=DEVICE))\n",
    "            print(\"Diffusion model loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading Diffusion model: {e}. Training from scratch.\")\n",
    "            if full_dataset and all_embeddings is not None:\n",
    "                 diffusion_model = train_diffusion_model(diffusion_model, full_dataset, optimizer_diffusion, DIFFUSION_EPOCHS, DIFFUSION_BATCH_SIZE, DEVICE, DIFFUSION_MODEL_PATH, all_embeddings)\n",
    "            else:\n",
    "                 print(\"Cannot train Diffusion model, missing data or embeddings.\")\n",
    "                 diffusion_model = None # Mark as None if training fails\n",
    "    else:\n",
    "        print(\"No pre-trained Diffusion model found. Training from scratch...\")\n",
    "        if full_dataset and all_embeddings is not None:\n",
    "            diffusion_model = train_diffusion_model(diffusion_model, full_dataset, optimizer_diffusion, DIFFUSION_EPOCHS, DIFFUSION_BATCH_SIZE, DEVICE, DIFFUSION_MODEL_PATH, all_embeddings)\n",
    "        else:\n",
    "            print(\"Cannot train Diffusion model, missing data or embeddings.\")\n",
    "            diffusion_model = None # Mark as None if training fails\n",
    "else:\n",
    "     print(\"Cannot initialize Diffusion model: EMBEDDING_DIM or NUM_EMOTIONS not set correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Data Augmentation using the Diffusion Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "augmentation-func"
   },
   "outputs": [],
   "source": [
    "def generate_augmented_data(diffusion_model, num_samples_per_emotion, target_emotion_label, all_embeddings, all_labels, device):\n",
    "    \"\"\"Generates augmented mel-spectrograms for a specific emotion.\"\"\"\n",
    "    if diffusion_model is None or all_embeddings is None or all_labels is None:\n",
    "        print(\"Diffusion model or embeddings/labels not available. Skipping augmentation.\")\n",
    "        return []\n",
    "\n",
    "    diffusion_model.eval()\n",
    "    generated_specs = []\n",
    "    print(f\"Generating {num_samples_per_emotion} samples for emotion '{target_emotion_label}'...\")\n",
    "\n",
    "    # Find embeddings corresponding to the target emotion\n",
    "    target_label_int = EMOTIONS.get(target_emotion_label)\n",
    "    if target_label_int is None:\n",
    "         print(f\"Error: Emotion '{target_emotion_label}' not found in EMOTIONS dictionary.\")\n",
    "         return []\n",
    "         \n",
    "    indices = np.where(all_labels == target_label_int)[0]\n",
    "\n",
    "    if len(indices) == 0:\n",
    "        print(f\"No embeddings found for emotion {target_emotion_label}. Cannot generate.\")\n",
    "        # Option: Use a generic embedding or average embedding? For now, skip.\n",
    "        return []\n",
    "\n",
    "    # Select embeddings to condition on (randomly pick from existing ones for that emotion)\n",
    "    selected_indices = np.random.choice(indices, num_samples_per_emotion, replace=True)\n",
    "    conditioning_embeddings = torch.from_numpy(all_embeddings[selected_indices]).float().to(device)\n",
    "\n",
    "    # Define the shape of the output spectrogram (Batch, Channel, Height, Width)\n",
    "    shape = (num_samples_per_emotion, 1, N_MELS, EXPECTED_TIME_STEPS)\n",
    "\n",
    "    # Generate using the sampling loop\n",
    "    with torch.no_grad():\n",
    "        generated_batch = p_sample_loop(\n",
    "            diffusion_model,\n",
    "            shape=shape,\n",
    "            timesteps=DIFFUSION_TIMESTEPS,\n",
    "            emotion_embeddings=conditioning_embeddings\n",
    "        )\n",
    "\n",
    "    # Move generated specs to CPU and store as numpy arrays\n",
    "    # Output shape is (B, C, H, W), need to store individual specs\n",
    "    generated_specs = [spec for spec in generated_batch.cpu().numpy()] \n",
    "\n",
    "    print(f\"Generated {len(generated_specs)} spectrograms for {target_emotion_label}.\")\n",
    "    return generated_specs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Perform Augmentation and Save Results\n",
    "\n",
    "Set `num_to_generate_per_emotion` to the desired number of synthetic samples per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "augmentation-exec"
   },
   "outputs": [],
   "source": [
    "# --- Perform Augmentation for Each Emotion ---\n",
    "num_to_generate_per_emotion = 50 # Adjust as needed (e.g., 50-100)\n",
    "augmented_metadata = []\n",
    "\n",
    "# Ensure augmented directory exists and is empty if re-running\n",
    "if os.path.exists(AUGMENTED_DIR):\n",
    "    print(f\"Clearing existing augmented data directory: {AUGMENTED_DIR}\")\n",
    "    # Be careful with rmtree!\n",
    "    # shutil.rmtree(AUGMENTED_DIR)\n",
    "    # Instead of deleting, let's just check and maybe skip generation\n",
    "    if os.listdir(AUGMENTED_DIR):\n",
    "         print(\"Warning: Augmented directory is not empty. Files might be overwritten or added.\")\n",
    "else:\n",
    "     os.makedirs(AUGMENTED_DIR, exist_ok=True)\n",
    "\n",
    "aug_metadata_path = os.path.join(DATA_DIR, \"augmented_metadata.npy\")\n",
    "\n",
    "# Check if metadata already exists to potentially skip generation\n",
    "if os.path.exists(aug_metadata_path):\n",
    "     print(f\"Augmented metadata file found at {aug_metadata_path}. Skipping generation.\")\n",
    "     try:\n",
    "          augmented_metadata = np.load(aug_metadata_path, allow_pickle=True).tolist()\n",
    "          print(f\"Loaded {len(augmented_metadata)} augmented metadata entries.\")\n",
    "     except Exception as e:\n",
    "          print(f\"Error loading augmented metadata: {e}. Will proceed with generation.\")\n",
    "          augmented_metadata = [] # Reset if loading failed\n",
    "else:\n",
    "     print(\"Augmented metadata not found. Proceeding with generation...\")\n",
    "     augmented_metadata = [] # Ensure it's empty before generation\n",
    "\n",
    "# Only generate if metadata list is empty\n",
    "if not augmented_metadata:\n",
    "    if diffusion_model is not None and all_embeddings is not None:\n",
    "        for emotion_name, emotion_int in EMOTIONS.items():\n",
    "            generated_spectrograms = generate_augmented_data(\n",
    "                diffusion_model,\n",
    "                num_to_generate_per_emotion,\n",
    "                emotion_name,\n",
    "                all_embeddings,\n",
    "                all_labels, # Pass all_labels here\n",
    "                DEVICE\n",
    "            )\n",
    "\n",
    "            # Save generated spectrograms and update metadata\n",
    "            for i, spec_np in enumerate(generated_spectrograms):\n",
    "                 # spec_np shape is likely (1, N_MELS, TIME), remove channel dim for saving\n",
    "                 if spec_np.ndim == 3 and spec_np.shape[0] == 1:\n",
    "                     spec_np = spec_np.squeeze(0)\n",
    "                 \n",
    "                 # Check shape before saving\n",
    "                 if spec_np.shape != (N_MELS, EXPECTED_TIME_STEPS):\n",
    "                      print(f\"Warning: Generated spec {i} for {emotion_name} has wrong shape {spec_np.shape}. Skipping save.\")\n",
    "                      continue\n",
    "                      \n",
    "                 save_filename = f\"augmented_{emotion_name}_{i:03d}.npy\"\n",
    "                 save_path = os.path.join(AUGMENTED_DIR, save_filename)\n",
    "                 try:\n",
    "                     np.save(save_path, spec_np.astype(np.float32))\n",
    "                     augmented_metadata.append((save_path, emotion_int))\n",
    "                 except Exception as e:\n",
    "                     print(f\"Error saving augmented spectrogram {save_path}: {e}\")\n",
    "\n",
    "        print(f\"\\nTotal augmented samples generated: {len(augmented_metadata)}\")\n",
    "        # Save augmented metadata only if generation occurred\n",
    "        if augmented_metadata:\n",
    "            np.save(aug_metadata_path, augmented_metadata)\n",
    "            print(f\"Saved augmented metadata to {aug_metadata_path}\")\n",
    "        else:\n",
    "            print(\"No augmented data was generated or saved.\")\n",
    "    else:\n",
    "        print(\"\\nSkipping data augmentation as diffusion model or embeddings are not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Evaluation\n",
    "\n",
    "Evaluate the performance of the trained ResNet classifier on:\n",
    "1. The original validation set.\n",
    "2. The newly generated augmented dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluation-func"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, criterion, device, num_classes):\n",
    "    \"\"\"Evaluates a classification model on a given dataset.\"\"\"\n",
    "    if model is None or data_loader is None or criterion is None:\n",
    "        print(\"Error: Model, dataloader, or criterion is None. Cannot evaluate.\")\n",
    "        return 0, 0, 0, 0, 0, np.zeros((num_classes, num_classes)), {}\n",
    "        \n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, _ in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Check if any samples were processed\n",
    "    dataset_size = len(data_loader.dataset)\n",
    "    if dataset_size == 0:\n",
    "         print(\"Error: Evaluation dataset is empty.\")\n",
    "         return 0, 0, 0, 0, 0, np.zeros((num_classes, num_classes)), {}\n",
    "         \n",
    "    avg_loss = total_loss / dataset_size\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "\n",
    "    # Calculate Metrics\n",
    "    wa = accuracy_score(all_targets, all_preds) # Weighted Accuracy (standard accuracy)\n",
    "    ua = balanced_accuracy_score(all_targets, all_preds) # Unweighted Accuracy\n",
    "    conf_matrix = confusion_matrix(all_targets, all_preds, labels=range(num_classes))\n",
    "    \n",
    "    # Get class names sorted by label index for the report\n",
    "    target_names_sorted = [name for name, idx in sorted(EMOTIONS.items(), key=lambda item: item[1])]\n",
    "    \n",
    "    class_report_dict = classification_report(all_targets, all_preds, labels=range(num_classes),\n",
    "                                        target_names=target_names_sorted,\n",
    "                                        zero_division=0, output_dict=True)\n",
    "    class_report_str = classification_report(all_targets, all_preds, labels=range(num_classes),\n",
    "                                        target_names=target_names_sorted,\n",
    "                                        zero_division=0)\n",
    "\n",
    "    print(f\"Evaluation Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Weighted Accuracy (WA): {wa:.4f}\")\n",
    "    print(f\"Unweighted Accuracy (UA): {ua:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(class_report_str)\n",
    "    # print(\"\\nConfusion Matrix:\")\n",
    "    # print(conf_matrix)\n",
    "\n",
    "    # Extract overall precision, recall, f1 (macro average)\n",
    "    precision = class_report_dict['macro avg']['precision']\n",
    "    recall = class_report_dict['macro avg']['recall']\n",
    "    f1 = class_report_dict['macro avg']['f1-score']\n",
    "\n",
    "    return wa, ua, precision, recall, f1, conf_matrix, class_report_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. Plot Confusion Matrix Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot-cm-func"
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, class_names, title='Confusion Matrix'):\n",
    "    \"\"\"Plots a confusion matrix using seaborn.\"\"\"\n",
    "    if cm is None or not isinstance(cm, np.ndarray):\n",
    "        print(\"Invalid confusion matrix provided for plotting.\")\n",
    "        return\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                annot_kws={\"size\": 10}) # Adjust font size if needed\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.title(title)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Get class names in the correct order for plotting\n",
    "class_names_sorted = [name for name, idx in sorted(EMOTIONS.items(), key=lambda item: item[1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3. Evaluate Original Data (Validation Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eval-original"
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- Evaluating on Original Validation Data ---\")\n",
    "wa_orig, ua_orig, p_orig, r_orig, f1_orig, cm_orig, cr_orig = (None,) * 7\n",
    "\n",
    "if resnet_model and val_loader and criterion_resnet:\n",
    "     wa_orig, ua_orig, p_orig, r_orig, f1_orig, cm_orig, cr_orig = evaluate_model(\n",
    "         resnet_model, val_loader, criterion_resnet, DEVICE, NUM_EMOTIONS\n",
    "     )\n",
    "     if cm_orig is not None:\n",
    "         print(\"\\nOriginal Data Evaluation Summary:\")\n",
    "         print(f\"WA: {wa_orig:.4f}, UA: {ua_orig:.4f}\")\n",
    "         print(f\"Precision (Macro): {p_orig:.4f}, Recall (Macro): {r_orig:.4f}, F1 (Macro): {f1_orig:.4f}\")\n",
    "         plot_confusion_matrix(cm_orig, class_names_sorted, title='Confusion Matrix (Original Validation Data)')\n",
    "     else:\n",
    "          print(\"Evaluation returned invalid confusion matrix.\")\n",
    "else:\n",
    "     print(\"ResNet model, validation loader, or criterion not available. Skipping original data evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4. Evaluate Generated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eval-generated"
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- Evaluating on Generated (Augmented) Data ---\")\n",
    "wa_aug, ua_aug, p_aug, r_aug, f1_aug, cm_aug, cr_aug = (None,) * 7\n",
    "\n",
    "# Load augmented metadata again (ensure it exists and is up-to-date)\n",
    "aug_metadata_path = os.path.join(DATA_DIR, \"augmented_metadata.npy\")\n",
    "augmented_metadata_loaded = []\n",
    "if os.path.exists(aug_metadata_path):\n",
    "    try:\n",
    "        augmented_metadata_loaded = np.load(aug_metadata_path, allow_pickle=True).tolist()\n",
    "        print(f\"Loaded {len(augmented_metadata_loaded)} augmented metadata entries for evaluation.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading augmented metadata for evaluation: {e}\")\n",
    "else:\n",
    "    print(f\"Augmented metadata file not found at {aug_metadata_path}. Cannot evaluate generated data.\")\n",
    "\n",
    "if augmented_metadata_loaded:\n",
    "    # Create dataset and dataloader for augmented data\n",
    "    aug_dataset = MelSpectrogramDataset(augmented_metadata_loaded)\n",
    "    if aug_dataset.num_classes != NUM_EMOTIONS and aug_dataset.num_classes > 0:\n",
    "         print(f\"Warning: Number of classes in augmented data ({aug_dataset.num_classes}) differs from original ({NUM_EMOTIONS}). Using {aug_dataset.num_classes} for evaluation.\")\n",
    "         eval_num_classes = aug_dataset.num_classes\n",
    "    elif aug_dataset.num_classes == 0:\n",
    "         print(\"Error: Augmented dataset has no classes. Cannot evaluate.\")\n",
    "         aug_dataset = None\n",
    "    else:\n",
    "         eval_num_classes = NUM_EMOTIONS\n",
    "         \n",
    "    if aug_dataset and len(aug_dataset) > 0:\n",
    "        # Use num_workers=0 if issues arise\n",
    "        aug_loader = DataLoader(aug_dataset, batch_size=RESNET_BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "        # Evaluate using the trained ResNet model\n",
    "        if resnet_model and criterion_resnet:\n",
    "            wa_aug, ua_aug, p_aug, r_aug, f1_aug, cm_aug, cr_aug = evaluate_model(\n",
    "                resnet_model, aug_loader, criterion_resnet, DEVICE, eval_num_classes\n",
    "            )\n",
    "            if cm_aug is not None:\n",
    "                 print(\"\\nGenerated Data Evaluation Summary:\")\n",
    "                 print(f\"WA: {wa_aug:.4f}, UA: {ua_aug:.4f}\")\n",
    "                 print(f\"Precision (Macro): {p_aug:.4f}, Recall (Macro): {r_aug:.4f}, F1 (Macro): {f1_aug:.4f}\")\n",
    "                 # Ensure class names match the number of classes evaluated\n",
    "                 plot_class_names = class_names_sorted[:eval_num_classes]\n",
    "                 plot_confusion_matrix(cm_aug, plot_class_names, title='Confusion Matrix (Generated Augmented Data)')\n",
    "            else:\n",
    "                 print(\"Evaluation returned invalid confusion matrix for augmented data.\")\n",
    "        else:\n",
    "            print(\"ResNet model or criterion not available. Skipping generated data evaluation.\")\n",
    "    else:\n",
    "         print(\"Augmented dataset is empty or invalid. Skipping evaluation.\")\n",
    "else:\n",
    "    print(\"No augmented data found to evaluate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5. Compare Results (Original vs. Augmented)\n",
    "This compares the classifier's performance on the held-out *original validation data* versus its performance on the *purely synthetic generated data*. This helps assess how well the generated data preserves recognizable emotional features according to the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "compare-results"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\\n--- Evaluation Comparison ---\")\n",
    "print(\"-\" * 30)\n",
    "print(\"| Metric          | Original(Val) | Generated |\")\n",
    "print(\"|-----------------|---------------|-----------|\")\n",
    "metric_available = wa_orig is not None and wa_aug is not None\n",
    "if metric_available:\n",
    "    print(f\"| WA              | {wa_orig:^13.4f} | {wa_aug:^9.4f} |\")\n",
    "    print(f\"| UA              | {ua_orig:^13.4f} | {ua_aug:^9.4f} |\")\n",
    "    print(f\"| Precision (Mac) | {p_orig:^13.4f} | {p_aug:^9.4f} |\")\n",
    "    print(f\"| Recall (Mac)    | {r_orig:^13.4f} | {r_aug:^9.4f} |\")\n",
    "    print(f\"| F1 Score (Mac)  | {f1_orig:^13.4f} | {f1_aug:^9.4f} |\")\n",
    "else:\n",
    "    print(\"| Results not available for comparison. |\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(\"\\nNote: Evaluation performed on original validation data vs. purely generated data.\")\n",
    "print(\"The paper's results might involve different evaluation splits or combining datasets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Conclusion\n",
    "\n",
    "This notebook implemented the core steps described in the paper \"A Generation of Enhanced Data by Variational Autoencoders and Diffusion Modeling\" for audio emotion data augmentation.\n",
    "\n",
    "1.  **Data Preparation:** Loaded and preprocessed EmoDB and RAVDESS datasets into standardized mel-spectrograms.\n",
    "2.  **Emotion Embedding:** Trained a ResNet-50 model for emotion classification and used it to extract emotion embeddings.\n",
    "3.  **Diffusion Model:** Implemented and trained a U-Net based diffusion model conditioned on time and emotion embeddings to generate mel-spectrograms.\n",
    "4.  **Data Augmentation:** Used the trained diffusion model to generate new mel-spectrograms for each target emotion.\n",
    "5.  **Evaluation:** Assessed the quality of the original and generated data using the trained ResNet classifier, comparing metrics like WA, UA, Precision, Recall, F1-score, and confusion matrices.\n",
    "\n",
    "The results can be compared to those in the paper (Table 5, Table 6, etc.), keeping in mind potential differences in dataset splits, specific model architectures, hyperparameters, and training duration. The diffusion model, in particular, often requires extensive training to generate high-fidelity samples.\n",
    "\n",
    "**Potential next steps:**\n",
    "*   Hyperparameter tuning for both ResNet and Diffusion models.\n",
    "*   Using more advanced attention mechanisms in the U-Net.\n",
    "*   Implementing more sophisticated diffusion sampling techniques (e.g., DDIM).\n",
    "*   Evaluating the impact of augmented data by adding it to the *training set* of the classifier and re-training/re-evaluating.\n",
    "*   Converting generated mel-spectrograms back to audio using a vocoder (e.g., HiFi-GAN, MelGAN - requires separate implementation/model) for auditory evaluation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
